---
output:
  pdf_document: default
  html_document: default
---

# Homework 2

Author: Dong Bin

email: bindong2@illinois.edu

# Question 1

## Part a
```{r}
rm(list = ls())
library(mlbench)
data(BostonHousing2)
BH = BostonHousing2[, !(colnames(BostonHousing2) %in% c("medv", "town", "tract"))]

# Get some basic informations
dim(BH)
names(BH)

# Fit a LM model
full.model <- lm(cmedv~., data = BH)
summary(full.model)
```

The most significant variables according to P value are:
1. rm

2. lstat

## Part b

```{r}
p <- dim(BH)[2]

test <- step(full.model, k = log(p))
```

The variables removed from full model after stepwise regression with BIC criteria are:
1. age

2. indus

3. lon

4. lat

# Part c
```{r}
library(leaps)
b = regsubsets(cmedv~ ., data = BH, nvmax =p)
rs = summary(b)
rs$which

```

# Part d
```{r}
row <- rs$which[1,]
names <- names(BH) 
xlabel <- c(1:15)
plot(x = xlabel, y = rs$cp, type="b", main="Mallow Cp of Different Subset Size", xlab = "Size of Subset", ylab = "Mallow Cp")

rs$which[11,]
```

The best model is when model size is 11.
Remaining variables are: crim, zn, chas1, nox, rm, dis, rad, tax, ptratio, b, lstat

```{r}
SubData <- BostonHousing2[, (colnames(BostonHousing2) %in% c("cmedv","crim", "zn", "chas1", "nox", "rm", "dis", "rad", "tax", "ptratio", "b", "lstat"))]
head(SubData)
summary(lm(cmedv~., data=SubData))
```

After removing insignificant variables, the most significant variables are :

1. rm

2. lstat

# Question 2

```{r}
rm(list=ls())
library(MASS)
set.seed(1)
n <- 200
p <- 200

# generate data
V <- matrix(0.2, p, p)
diag(V) <- 1
X <- as.matrix(mvrnorm(n, mu = rep(0, p), Sigma = V))
y <- X[, 1] + 0.5*X[, 2] + 0.25*X[, 3] + rnorm(n)
  
# we will use a scaled version 
X <- scale(X)
y <- scale(y)
```

## Part a

```{r}
soft_th <- function(b, lambda){
  sign(b) * max(abs(b)-lambda/2, 0);
}

part_a_plot_x <- c(-10:10)
part_a_plot_y <- list()
for(i in c(1:length(part_a_plot_x)))
{
  part_a_plot_y[i] <- soft_th(part_a_plot_x[i], 5)
}
plot(part_a_plot_x, part_a_plot_y, type <- "b")
```

## Part b
```{r}
beta_old <- rep(0, p)
update_beta <- function(beta, lambda, X, y, printable)
{
  current_beta <- beta;
  r <- y-X%*%beta
  for(i in 1:length(current_beta))
  {
    r = r+ X[,i]*current_beta[i]
    current_beta[i] = soft_th(X[,i]%*%r, lambda*ncol(X))/(t(X[,i]%*%X[,i]))
    r = r-X[,i]*current_beta[i]
    r <- y-X%*%current_beta
    if(i<=3 && printable)
    {
      cat("Obervation of r: ",  i, "\n")
      print(c(r))
    }
  }
  return <-current_beta
}
a = update_beta(beta_old, 0.7, X, y, TRUE)
cat("Value of beta after one loop: \n")
print(a)
```

## Part c
```{r}
myLasso <- function(X, y, lambda, tol, maxitr)
{
  old_beta = rep(0, p);
  for( i in 1:maxitr)
  {
    cat("Iteration: ", i, "\n")
    new_beta = update_beta(old_beta, lambda, X, y, FALSE);
    residule = sum(abs(new_beta-old_beta))
    if(i<=3)
    {
      print(residule)
    }
    if(residule < tol){
      cat("Terminated after ", i, " ietrations", "\n")
      break;
    }
    
    
    old_beta = new_beta;
  }
  return <- old_beta
}

myfit <- myLasso(X, y, 0.3, 1e-10, 100)
for(i in 1:length(myfit))
{
  if(myfit[i]!=0)
  {
    cat(i, myfit[i], "\n");
  }
}
```


## Part d
```{r}
library(glmnet)
lasso = glmnet(X, y, alpha=1, lambda=0.15)
summary(lasso$beta)

distance <- sum(abs(myfit-lasso$beta))
cat("The distance between my implementation and glmnet is: ", distance, "\n")
```


# Question 3

## Part a

```{r}
rm(list=ls())
read.csv('Train.csv', header=TRUE)


```