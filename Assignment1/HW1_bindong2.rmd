---
output:
  pdf_document: default
  html_document: default
---
# Question 1

**Author: Bin Dong**

**Email: bindong2@iillinois.edu**

## a. 
```{r}
# Clear entire workspace
rm(list=ls())

# l2 norm distance
mydist <- function(x1, x2) {
    #data <- t(apply(x2, 1, function(x) x-x1))
    data <- sweep(x2, 2, x1);
    return (sqrt(apply((data)^2, 1, sum)))
}

# Get most frequent 
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

# KNN
myknn <- function(xtest, xtrain, ytrain, k) {
    # Initialte a vector for storing results.
    result <- rep(0, times = nrow(xtest))
    for( c in (1:nrow(xtest)))
    {
      dimMatrix <- mydist(xtest[c,],xtrain)
      # Sort by indices and take the order of them (top k)
      top_indices = order(dimMatrix, decreasing=FALSE)[1:k]
      result[c] <- mean(ytrain[top_indices])
    }
    return <- result
}
```

```{r}

# This section is only a test of functionality of myknn.
# xtest is 2 by 5 matrix
xtest <- matrix(1:10, ncol=5, byrow=TRUE)
# xtrain is 20 by 5 matrix
xtrain <- matrix(1:100, ncol=5, byrow=TRUE)
# ytrain is 20 length vector
ytrain <- c(1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0)

# a is expected as a 2 length vector
a<- myknn(xtest, xtrain, ytrain, 3)
print(a)
```
## b. 
```{r}
# Initialize random seed
set.seed(1)

# Use library MASS for generating multi dimension normal distribution
library(MASS)

# Mean
my_mean <- c(1,2,3,4,5)
my_var <- matrix(rep(0, 25), nrow=5, ncol=5)
for( i in c(1:5))
{
  for(j in c(1:5))
  {
    my_var[i,j] = 0.5^(abs(i-j))
  }
}

x <- mvrnorm(1000,my_mean,my_var)

y <- x[,1]+x[,2]+(x[,3]-2.5)^2+rnorm(1,0,1)
```
The first three data are:
```{r}
print(x[1:3,])
print(y[1:3])
```
## c.
```{r}
split_count <- 400
x_train <- x[1:split_count,]
x_test <- x[(split_count+1):1000,]
y_train <- y[1:split_count]
y_test <- y[(split_count+1):1000]

predictions <- myknn(x_test, x_train, y_train, 5)
```

The MSE when k=5 is:
```{r}
sum((predictions - y_test)^2)/600

```

## d.
```{r}
k_collection <- c(1,2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100)

y_collection <- rep(0, times = length(k_collection))


# Collecting MSE for different k
y_index <- 1

for(k in k_collection)
{
  predictions <- myknn(x_test, x_train, y_train, k)
  y_collection[y_index] <- sum((predictions - y_test)^2)/600
  y_index <- y_index+1
}

# Fit data with a linear model.
myfit <- lm(y_train~x_train)

xtest.df <- data.frame(x_train=I(x_test))
lmprediction <- predict(myfit, newdata=xtest.df)

lm.mse <- sum((y_test- lmprediction)^2)/nrow(x_test)

lm_benchmark = rep(lm.mse, times = length(k_collection))

# Plot data. y_collection is mse of knn and lm_benchmark is mse of linear model.
plot(k_collection, y_collection,type="b", xlab = "K", ylab="accuracy(mean squared error)", col ="red", lty = 1)

lines(k_collection, lm_benchmark, type="l", col="blue", lty=2)
legend("topleft", cex=.6, c("knn", "lm"), col=c("red", "blue"), lty=1:2)
```

# Question 2

## a.
```{r}
rm(list=ls())

# Calculate gradient
gradient <- function (x, y, beta)
{
  return <- as.vector(t(x%*%beta-y)%*%x/nrow(x))
}

# optimization function
mylm_g <- function(X, Y, delta, epsilon, maxiter)
{
  beta <- rep(0, times=ncol(X))
  epsilon <- rep(epsilon, times=ncol(X))
  iter <- 0
  for(i in c(1:maxiter)){
    oldBeta <- beta;

    newBeta <- oldBeta - gradient(X,Y,oldBeta)*delta
    
    if(all(newBeta-oldBeta < epsilon))
    {
      
      cat("stopped after", i, "iterations", "\n")
      break;
    }
    beta<-newBeta
    
  }
  return <- beta;
}

```

## b.
```{r}

library(mlbench)
data(BostonHousing2)
X = BostonHousing2[, !(colnames(BostonHousing2) %in% c("medv", "town", "tract", "cmedv"))]
X = data.matrix(X)
X = scale(X)
Y = as.vector(scale(BostonHousing2$cmedv))
beta = rep(0, times=15)

result <- mylm_g(X, Y, 0.1, 0.000001, 10000)

print(result)

# Compare with build-in lm() function
myfit <- lm(Y~X)
print(myfit)
```


After comparing parameters, they are very close to each other between built-in lm() function and my implementation.

## Bonus Question

```{r}
library(mlbench)
data(BostonHousing2)
X = BostonHousing2[, !(colnames(BostonHousing2) %in% c("medv", "town", "tract", "cmedv"))]
X = data.matrix(X)
Y = as.vector(BostonHousing2$cmedv)
beta = rep(0, times=15)

result <- mylm_g(X, Y, 0.000001, 0.000001, 100000)

print(result)

# Compare with build-in lm() function
myfit <- lm(Y~X)
print(myfit)
```

The first observation is that it cannot converge when delta is large.

THe second observation is that it is not converging to the same result as lm.

The reasons behind this behavior are:

1. The way i used to converge gradient is to substract a uniform vector from beta. When data are not scaled, their parameters are in different orders of magnitude. Substracting a uniform vector from such parameters can really slow down the converging of large parameters.
2. Since large parameters are dominating the gradient, so small parameters will not converge if the step is set too large.